`torchrun` 是 **PyTorch** 提供的一个命令行工具，用于**启动分布式训练任务**

 可以完成：

* 自动设置分布式训练需要的环境变量
* 负责进程组的启动(本地多进程和跨节点多进程)
* 代码中只需要使用`torch.distributed.init_process_group`就能跑在多GPU/多机器上

```python
import torch
import torch.distributed as dist

dist.init_process_group("nccl")		// 初始化分布式环境，后端指定nccl
nrank = dist.get_world_size()		// 返回分布式作业的总进程数，启动参数--nproc-per-node=2
rank = dist.get_rank()				// 当前进程在全局进程组的编号

device_id = rank % torch.cuda.device_count()
torch.cuda.set_device(device_id)	// 当前进程绑定GPU
```

gemm_rs_ring.py

```python
# 获取默认的进程组（ProcessGroupNCCL），用于做分布式通信
op = gemm_rs_op.gemm_rs_op(
    M, N, torch.float16, device_id, dist.distributed_c10d._get_default_group()
)

# 等待所有进程同步后才开始
torch.distributed.barrier()

# 确保所有cuda操作完成
torch.cuda.synchronize()
```



