看每个example，然后记录一下学到的新用法

# 00_basic_gemm

最基本的流程

```c++
// 1. 定义数据类型和内存布局
using CutlassGemm = cutlass::gemm::device::Gemm<...>

// 2、创建cutlass gemm运算对象
CutlassGemm gemm_operator;

// 3、cutlass gemm 参数对象
CutlassGemm::Arguments args(...)
    
// 4、启动kernel
cutlass::Status status = gemm_operator(args);
```

* 主维度跨度（Leading Dimension）

  描述的是 **在内存中相邻列（或行）之间的距离**，怎么理解呢？

  **每个主方向（列或行）在内存中相邻两个元素之间的间隔**

  对于列主序(ColumnMajor)，ld是实际分配的行数，也就是列之间跳跃的距离

  对于行主序，ld是列数，也就是行之间跳跃的距离

```c++
// A 为 M×K 的列主序矩阵（每列连续）
float A[lda * K]; // lda >= M
// 第i行第j列
A[i + j * lda]

// 举例，3行2列
A = [1   4
     2   5
     3   6]

// 列主序的内存布局[1 2 3 4 5 6]
// 如果我们设置ld=4, 则会填充数据，实际内存布局是[1 2 3 x 4 5 6 x] x是预留或未用空间
// 访问第i行第j列:A[i + j * ld]

// 行主序的内存布局[1 4 2 5 3 6]
// 如果我们设置ld=3, 则会填充数据，实际内存布局是[1 4 x 2 5 x 3 6 x] x是预留或未用空间
// 访问第i行第j列:A[i * ld + j]
    
// Q&A 在矩阵乘法的时候，会有TN和TT等，对于A(1024, 4096), B（4096， 2048），TN乘法对应的lda和ldb分别是？
```



# 01_cutlass_utilities

cutlass tools/util目录下提供了一些实用的方法，不太习惯的是cutlass在根目录下的很多子目录下都有include

```bash
-I/code/cutlass/include -I/code/cutlass/examples/common -I/code/cutlass/build/include -I/code/cutlass/tools/util/include -isystem "/usr/local/cuda-11.5/include"
```

* 数值类型

  ```c++
  cutlass::half_t
  // 这是一个实现 IEEE 半精度数值的数字类型。它在主机和设备代码中均可用
  // include/cutlass/numeric_types.h
  #include "cutlass/integer_subbyte.h"
  #include "cutlass/half.h"
  #include "cutlass/bfloat16.h"
  #include "cutlass/tfloat32.h"
  #include "cutlass/float8.h"
  #include "cutlass/uint128.h"
  ```

* cutlass::HostTensor<>

此模板类简化了所有受支持布局的张量创建。它简化了主机和设备内存分配的分配和管理。

此类提供方法 device_view() 和 host_view()，用于为设备和主机端内存分配提供 TensorView 对象

* cutlass::reference::device::TensorFillRandomGaussian()

此模板函数将张量的元素初始化为随机高斯分布

```c++
cutlass::reference::device::TensorFillRandomGaussian(
    A.device_view(),
    seed,
    mean,
    stddev,
    bits_less_than_one
);
```

* cutlass::reference::host::Gemm<>

```c++
  using Gemm = cutlass::gemm::device::Gemm<
    cutlass::half_t,                           // ElementA
    cutlass::layout::ColumnMajor,              // LayoutA
    cutlass::half_t,                           // ElementB
    cutlass::layout::ColumnMajor,              // LayoutB
    cutlass::half_t,                           // ElementOutput
    cutlass::layout::ColumnMajor               // LayoutOutput
  >;

  Gemm gemm_op;
  
  cutlass::Status status = gemm_op({
    {M, N, K},
    {A, lda},
    {B, ldb},
    {C, ldc},
    {C, ldc},
    {alpha, beta}
  });
```



我的理解是，这是cutlass提供的主机端的gemm实现

* cutlass::reference::host::TensorEquals()

用于比较两个具有相同秩的张量是否相等

```c++
// 张量分配在 设备（GPU）内存中，但控制权在主机端（HostTensor 结构）
cutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> A(cutlass::MatrixCoord(M, K));
cutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> B(cutlass::MatrixCoord(K, N));
cutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> C_cutlass(cutlass::MatrixCoord(M, N));
cutlass::HostTensor<cutlass::half_t, cutlass::layout::ColumnMajor> C_reference(cutlass::MatrixCoord(M, N));

...
// 填充矩阵 A/B/C_cutlass 的设备数据
cutlass::reference::device::TensorFillRandomGaussian(
    A.device_view(),
    seed,
    mean,
    stddev,
    bits_less_than_one
    );

cutlass::reference::device::TensorFillRandomGaussian(
    B.device_view(),
    seed * 2019,
    mean,
    stddev,
    bits_less_than_one
    );

cutlass::reference::device::TensorFillRandomGaussian(
    C_cutlass.device_view(),
    seed * 1993,
    mean,
    stddev,
    bits_less_than_one
    );

cutlass::device_memory::copy_device_to_device(
    C_reference.device_data(), 
    C_cutlass.device_data(), 
    C_cutlass.capacity());

  // Compute the reference result using the host-side GEMM reference implementation.
  cutlass::reference::host::Gemm<
    cutlass::half_t,                           // ElementA
    cutlass::layout::ColumnMajor,              // LayoutA
    cutlass::half_t,                           // ElementB
    cutlass::layout::ColumnMajor,              // LayoutB
    cutlass::half_t,                           // ElementOutput
    cutlass::layout::ColumnMajor,              // LayoutOutput
    cutlass::half_t,
    cutlass::half_t
  > gemm_ref;

  gemm_ref(
    {M, N, K},                          // problem size (type: cutlass::gemm::GemmCoord)
    alpha,                              // alpha        (type: cutlass::half_t)
    A.host_ref(),                       // A            (type: TensorRef<half_t, ColumnMajor>)
    B.host_ref(),                       // B            (type: TensorRef<half_t, ColumnMajor>)
    beta,                               // beta         (type: cutlass::half_t)
    C_reference.host_ref()              // C            (type: TensorRef<half_t, ColumnMajor>)
  );

```

# 02 dump_reg_shmem

介绍了用于调试gmem、smem以及fragment的方法

* 初始化张量并填充

  ```c++
  cutlass::HostTensor<Element, Layout> matrix(
    {EXAMPLE_MATRIX_ROW, EXAMPLE_MATRIX_COL});
  cutlass::reference::host::BlockFillSequential(matrix.host_data(),
                                              matrix.capacity());
  // 打印一个hosttensor，说明TensorView实现了operator[]
  std::cout << "Matrix:\n" << matrix.host_view() << "\n";
  
  // H2D
  matrix.sync_device();
  
  ```

* 迭代器

```c++
  // gmem迭代器，定义一个线程块的内迭代器，用来从gmem中按照tile访问数据
  // MatrixShape : tile的形状
  // ThreadMap：描述线程如何在 tile 中分工（哪个线程负责加载 tile 的哪一部分）
  using GmemIterator =
      cutlass::transform::threadblock::PredicatedTileIterator<
          cutlass::MatrixShape<EXAMPLE_MATRIX_ROW, EXAMPLE_MATRIX_COL>, Element,
          Layout, 1, ThreadMap>;
  
  // 构造 GmemIterator 所需的 layout 参数
  typename GmemIterator::Params params(matrix.layout());

  // 共享内存迭代器
  using SmemIterator = cutlass::transform::threadblock::RegularTileIterator<
      cutlass::MatrixShape<EXAMPLE_MATRIX_ROW, EXAMPLE_MATRIX_COL>, Element,
      cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous<16, 64>, 1,
      ThreadMap>;

  // 使用给定参数构造全局内存迭代器，加载块的形状为{EXAMPLE_MATRIX_ROW, EXAMPLE_MATRIX_COL}
  GmemIterator gmem_iterator(params, ref.data(),
                             {EXAMPLE_MATRIX_ROW, EXAMPLE_MATRIX_COL},
                             tb_thread_id);
```

* 打印线程寄存器的数据

```c++
  // 加载小块数据到寄存器
  typename GmemIterator::Fragment frag;
  frag.clear();
  gmem_iterator.load(frag);

  // 打印每个线程寄存器拿到的数据，每个线程拿到了8*8的数据。每次迭代拿到8个元素，总共迭代8次
  cutlass::debug::dump_fragment(frag);

  cutlass::debug::dump_fragment(frag, /*N = */ 1);    // 只打印第一个线程的片段

  cutlass::debug::dump_fragment(frag, /*N = */ 1, /*M = */ 16); // 打印前 16 个元素， 0 1 2 3 4 5 6 7 256 257 258 259 260 261 262 263

  cutlass::debug::dump_fragment(frag, /*N = */ 1, /*M = */ 16, /*S = */ 8); // 前16个，打印时步长为 8，所以就是0, 256
```

* 使用共享内存迭代器

```
  // 构造共享内存迭代器， 使用 store 将片段写入共享内存
  SmemIterator smem_iterator(
      typename SmemIterator::TensorRef(
          {shared_storage, SmemIterator::Layout::packed(
                               {EXAMPLE_MATRIX_ROW, EXAMPLE_MATRIX_COL})}),
      tb_thread_id);

  // 将上面寄存器的数据写到共享内存中
  smem_iterator.store(frag);
```

* 打印共享内存的值

```c++
 cutlass::debug::dump_shmem(shared_storage,
                             EXAMPLE_MATRIX_ROW * EXAMPLE_MATRIX_COL);
 
// 打印共享内存值，步长8，这里只打印了一次，应该是封装里面限定了一个线程打印
 cutlass::debug::dump_shmem(
      shared_storage, EXAMPLE_MATRIX_ROW * EXAMPLE_MATRIX_COL, /*S = */ 8); 
```

# 03、visualization tool

介绍了一下cutlass提供的布局可视化工具，暂时跳过

行主序和列主序

![image-20250703003201986](C:\Users\Qufanhua\AppData\Roaming\Typora\typora-user-images\image-20250703003201986.png)



# 04、tile_iterator

使用PredicatedTileIterator将数据从一个地址搬运到另一个地址。PredicateTileIterator 接受 ThreadMap 类型，该类型**定义了线程到内存中“tile”的映射**

TileIterator 是 CUTLASS 中的一个核心概念，它能够高效地实现数据加载

```c++
template <typename Iterator>
__global__ void copy(
    typename Iterator::Params dst_params,
    typename Iterator::Element *dst_pointer,
    typename Iterator::Params src_params,
    typename Iterator::Element *src_pointer,
    cutlass::Coord<2> extent) {

    // 每个线程根据 threadIdx.x 构造自己的 src 和 dst tile iterator。
    // 每个线程会操作它对应的 tile 中的数据
    Iterator dst_iterator(dst_params, dst_pointer, extent, threadIdx.x);
    Iterator src_iterator(src_params, src_pointer, extent, threadIdx.x);

    // PredicatedTileIterator uses PitchLinear layout and therefore takes in a PitchLinearShape.
    // The contiguous dimension can be accessed via Iterator::Shape::kContiguous and the strided
    // dimension can be accessed via Iterator::Shape::kStrided
    // 计算迭代次数
    int iterations = (extent[1] + Iterator::Shape::kStrided - 1) / Iterator::Shape::kStrided;

    // 初始化fragment
    typename Iterator::Fragment fragment;
    for(size_t i = 0; i < fragment.size(); ++i) {
      fragment[i] = 0;
    }

    src_iterator.load(fragment);	// 从源地址拷贝到fragment
    
    // 打印第一次的数据
    cutlass::debug::dump_fragment(fragment);
    
    dst_iterator.store(fragment);	// 从fragment拷贝到目标地址


    ++src_iterator;
    

    
    ++dst_iterator;

    for(; iterations > 1; --iterations) {

      src_iterator.load(fragment);
      dst_iterator.store(fragment);

      ++src_iterator;
      ++dst_iterator;
    }
}

using Shape = cutlass::layout::PitchLinearShape<64, 4>;
using Layout = cutlass::layout::PitchLinear;    // 使用 PitchLinear layout。
using Element = int;
int const kThreads = 32;  // 使用32 个线程。

// 定义了线程和tile的映射关系
using ThreadMap = cutlass::transform::PitchLinearStripminedThreadMap<Shape, kThreads>;
using Iterator = cutlass::transform::threadblock::PredicatedTileIterator<Shape, Element, 					Layout, 1, ThreadMap>;

// 在这个例子里面， M = 57, N = 35

// 将 dst_tensor 填充为 -1（用于检测越界）
// 将 src_tensor 填充为递增整数
cutlass::reference::host::TensorFill(dst_tensor.host_view(), oob_value);
cutlass::reference::host::BlockFillSequential(src_tensor.host_data(), src_tensor.capacity());

std::cout << "src_tensor = \n" << src_tensor.host_view() << "\n";    // 列存数据

// H2D
dst_tensor.sync_device();
src_tensor.sync_device();

typename Iterator::Params dst_params(dst_tensor.layout());
typename Iterator::Params src_params(src_tensor.layout());

```

![image-20250703010929195](C:\Users\Qufanhua\AppData\Roaming\Typora\typora-user-images\image-20250703010929195.png)

第一次取的数据
![image-20250703011205217](C:\Users\Qufanhua\AppData\Roaming\Typora\typora-user-images\image-20250703011205217.png)

第二次取的数据
![image-20250704002425967](C:\Users\Qufanhua\AppData\Roaming\Typora\typora-user-images\image-20250704002425967.png)

这里面的填充挺有意思的，大块的形状是57行35列，每次取的小块是64行4列，总共取了9次，然后在第一次取地】的末尾补了0



# 05_batched_gemm

展示了通过cutlass以两种方式计算batch gemm

1、通过指定第一个矩阵的指针以及batch中的步长(stride)，称为strided batched gemm

2、将所有矩阵的指针拷贝到设备内存，称为array gemm



```c++
/*
In this example, both A and B matrix are non-transpose and column major matrix
batched_C = batched_A x batched_B
As an example, matrix C can be seen as
-----------------------------------------------------------
(0,0,0) | (0,0,1) | (0,0,2) | (1,0,0) | (1,0,1) | (1,0,2) |
-----------------------------------------------------------
(0,1,0) | (0,1,1) | (0,1,2) | (1,1,0) | (1,1,1) | (1,1,2) |
-----------------------------------------------------------
(0,2,0) | (0,2,1) | (0,2,2) | (1,2,0) | (1,2,1) | (1,2,2) |
-----------------------------------------------------------
(0,3,0) | (0,3,1) | (0,3,2) | (1,3,0) | (1,3,1) | (1,3,2) |
-----------------------------------------------------------
(0,4,0) | (0,4,1) | (0,4,2) | (1,4,0) | (1,4,1) | (1,4,2) |
-----------------------------------------------------------
(0,5,0) | (0,5,1) | (0,5,2) | (1,5,0) | (1,5,1) | (1,5,2) |
-----------------------------------------------------------
           batch 0          |           batch 1

matrix A can be seen as
---------------------------------------
(0,0,0) | (0,0,1) | (1,0,0) | (1,0,1) |
---------------------------------------
(0,1,0) | (0,1,1) | (1,1,0) | (1,1,1) |
---------------------------------------
(0,2,0) | (0,2,1) | (1,2,0) | (1,2,1) |
---------------------------------------
(0,3,0) | (0,3,1) | (1,3,0) | (1,3,1) |
---------------------------------------
(0,4,0) | (0,4,1) | (1,4,0) | (1,4,1) |
---------------------------------------
(0,5,0) | (0,5,1) | (1,5,0) | (1,5,1) |
---------------------------------------
     batch 0      |      batch 1
     
matrix B can be seen as
-----------------------------
(0,0,0) | (0,0,1) | (0,0,2) |
----------------------------- batch 0
(0,1,0) | (0,1,1) | (0,1,2) |
-------------------------------------
(1,0,0) | (1,0,1) | (1,0,2) |
----------------------------- batch 1
(1,1,0) | (1,1,1) | (1,1,2) |
-----------------------------

*/

// 方法二 
cudaError_t cutlass_array_sgemm(
  int m,
  int n,
  int k,
  float alpha,
  float const * const *A,		// 注意这里是存有指针的数组，对应每个批次矩阵的指针
  int lda,  // 6 yes			// 列主序下是行数
  float const * const *B,
  int ldb,  // 2  no 2193 why?	 这里是因为B的分batch是在行方向的，所以stride是129*17
  float * const *C,
  int ldc,  // 6 yes
  float beta,
  int batch_count) {

  // A、B、C都是列主序
  using Gemm = cutlass::gemm::device::GemmArray<
    float, cutlass::layout::ColumnMajor,
    float, cutlass::layout::ColumnMajor,
    float, cutlass::layout::ColumnMajor
  >;

  Gemm gemm_op;

  cutlass::Status status = gemm_op({
    {m, n, k},
    A, lda,
    B, ldb,
    C, ldc,
    C, ldc,
    {alpha, beta},
    batch_count
  });

  if (status != cutlass::Status::kSuccess) {
    return cudaErrorUnknown;
  }

  return cudaSuccess;
}

// 调用
  // Arbitrary problem size
  int const m = 520;
  int const n = 219;
  int const k = 129;
  int const batch_count = 17;   // 随便取的

  int const lda = m;
  int const ldb = k * batch_count; 
  int const ldc = m;

  int const count_A = batch_count * lda * k;
  int const count_B = ldb * n;
  int const count_C = batch_count * ldc * n;

  // alpha and beta
  float alpha = 1.0f;
  float beta = 2.0f;

  // allocate the host memory
  std::vector<float> host_A(count_A);
  std::vector<float> host_B(count_B);
  std::vector<float> host_C(count_C);
  std::vector<float> result_C(count_C);

  // allocate the device memory
  float *A;
  float *B;
  float *C;

// malloc
  cudaMalloc(&A, count_A * sizeof(float));
  cudaMalloc(&B, count_B * sizeof(float));
  cudaMalloc(&C, count_C * sizeof(float));

  // Limit range to avoid floating-point errors
  int const kRange = 8;

  // fill A
  for (int b_idx = 0; b_idx < batch_count; b_idx++) {
    for (int col_idx = 0; col_idx < k; col_idx++) {
      for (int row_idx = 0; row_idx < m; row_idx++) {
        host_A[row_idx + col_idx * lda + b_idx * lda * k] = static_cast<float>((row_idx + col_idx * lda + b_idx * lda * k) % kRange);
      }
    }
  }
// fill B and C
......
// H2D copy
......
	// 存储数组指针的数组，主机端
    std::vector<float*> host_ptr_A(batch_count);
    std::vector<float*> host_ptr_B(batch_count);
    std::vector<float*> host_ptr_C(batch_count);

    // 打乱顺序，来说明通过这个方法，不依赖于连续步长，在主机端分配
    std::vector<size_t> permutation = {14, 11, 3, 10, 1, 13, 9, 4, 6, 16, 8, 15, 7, 12, 0, 2, 5};
    for (size_t b_idx = 0; b_idx < batch_count; b_idx++) {
      host_ptr_A[b_idx] = A + permutation[b_idx] * batch_stride_A;
      host_ptr_B[b_idx] = B + permutation[b_idx] * batch_stride_B;
      host_ptr_C[b_idx] = C + permutation[b_idx] * batch_stride_C;
    }

// 设备端
    float const **ptr_A;
    float const **ptr_B;
    float **ptr_C;
// allocate
......
// H2D

// 调用，所以方法二和普通的gemm没有区别，可以认为只是在执行一个循环
result = cutlass_array_sgemm(m, n, k, alpha, ptr_A, lda, ptr_B, ldb, ptr_C, ldc, beta, batch_count);

// 方法1：

cudaError_t cutlass_strided_batched_sgemm(
  int m, 
  int n,
  int k,
  float alpha,
  float const *A,
  int lda,
  long long int batch_stride_A,
  float const *B,
  int ldb,
  long long int batch_stride_B,
  float *C,
  int ldc,
  long long int batch_stride_C,
  float beta,
  int batch_count) {

  using Gemm = cutlass::gemm::device::GemmBatched<
    float, cutlass::layout::ColumnMajor,
    float, cutlass::layout::ColumnMajor,
    float, cutlass::layout::ColumnMajor
  >;

  Gemm gemm_op;

  cutlass::Status status = gemm_op({
    {m, n, k},
    {A, lda}, 
    batch_stride_A,
    {B, ldb}, 
    batch_stride_B,
    {C, ldc}, 
    batch_stride_C,
    {C, ldc}, 
    batch_stride_C,
    {alpha, beta},
    batch_count
  });

// 调用，方法1的话就只能按照指定步长去执行，方法二更灵活一些，但是需要主机端去做一个排布，有额外的现存开销(较小)
  long long int batch_stride_A = static_cast<long long int>(lda) * static_cast<long long int>(k);
  long long int batch_stride_B = static_cast<long long int>(k);
  long long int batch_stride_C = static_cast<long long int>(ldc) * static_cast<long long int>(n);

    result = cutlass_strided_batched_sgemm(
      m, n, k, alpha, A, lda, batch_stride_A, B, ldb, batch_stride_B, C, ldc, batch_stride_C,
      beta, batch_count);

```

# 06、splitk_gemm

什么是split-K?

对于一个规模为(M,N,K) = （128,128,4096）的gemm，假如一个block大小为(128,128,4096)，那这样一个block就可以完成这个gemm，但是由于block只能执行在一个SM上，这样效率非常低。

split-k 就派上用场了。它是一种将 K 维矩阵乘法划分并分布到多个 SM 上的方法，可以获得比单个 SM 更高的效率。在上面的例子中，我们可以使用 split-k 因子 16 来划分 K 维，即线程块块大小为 128x128x256。

CUTLASS 将内核划分为可组合的层级结构；在每个线程、warp 和线程块级别，它们都基于各自的 tile 大小进行计算，而较高级别的 tile 大小由较低级别的 tile 大小组成。



下面例子中变量初始化可以划分为

* 设置数据属性：数据在内存中的layout，以及内核如何查看(逻辑到物理的映射)
* 设置计算属性：如何使用上述数据计算得到输出

对于问题D = alpha * A * B + beta * C

CUTLASS 会先完成 `A * B` 的乘法运算，然后将结果记作 `X`，最后执行D = alpha * X + beta * C。这个 `alpha * X + beta * C` 的阶段称为 **epilogue**（后处理），做的事轻量的逐元素计算(element-wise)

模板参数配置：

```c++
ElementAccumulator = float
ElementComputeEpilogue = float
ElementInputA = cutlass::half_t
ElementInputB = cutlass::half_t
ElementOutput = float
```

内存布局

```c++
LayoutInputA = column major	// A是列主序
LayoutInputB = row major	// B是行主序
LayoutOutput = row major	// 输出也是行主序
```

配置 epilogue 运算，在这个例子中，使用

```c++
cutlass::epilogue::thread::LinearCombination
```

计算属性：

block(128,128,32)、warp(64,64,4)、mma-op tile(8,8,4)，这些参数传递给cutlass gemm内核实例化时，会在内部推断每个线程块需要的线程数量、smem大小、并且以没有内存bank冲突的方式存储数据。除此之外，还会构建、初始化和启动gemm内核需要的其他变量。原本复杂繁琐的工作变得更简单了，开发人员可以更轻松地定制需要的gemm内核，而无需将过多精力用于理解和编写优化代码。

在这个例子中使用`cutlass:device::GemmSplitKParallel`模板来描述gemm内核，Volta架构

```c++
// 数据类型
using ElementAccumulator = float;                   // <- data type of accumulator
using ElementComputeEpilogue = ElementAccumulator;  // <- data type of epilogue operations
using ElementInputA = cutlass::half_t;              // <- data type of elements in input matrix A
using ElementInputB = cutlass::half_t;              // <- data type of elements in input matrix B
using ElementOutput = float;                        // <- data type of elements in output matrix D

// layout
using LayoutInputA = cutlass::layout::ColumnMajor;
using LayoutInputB = cutlass::layout::RowMajor;
using LayoutOutput = cutlass::layout::RowMajor;

// 选择使用tensor core，而不是普通的SIMT指令(cutlass::arch::OpClassSimt)
using MMAOp = cutlass::arch::OpClassTensorOp;

// 指定架构
using SmArch = cutlass::arch::Sm70;

// 描述各个层级处理的tile大小
using ShapeMMAThreadBlock = cutlass::gemm::GemmShape<128, 128, 32>;
using ShapeMMAWarp = cutlass::gemm::GemmShape<64, 64, 32>;
using ShapeMMAOp = cutlass::gemm::GemmShape<8, 8, 4>;

// 定义epilogue类型
using EpilogueOp = cutlass::epilogue::thread::LinearCombination<
    ElementOutput,                                     // 输出类型
// 128 是硬件向量寄存器的 bit 宽度（128-bit），这是硬件优化的单位
    128 / cutlass::sizeof_bits<ElementOutput>::value,  // 每个每次向量化存取的元素个数  
    ElementAccumulator,                                // 累加器数据类型
    ElementComputeEpilogue>;  // alpha和beta的数据类型

// 创建一个GemmSplitKParallel模板实例
using Gemm = cutlass::gemm::device::GemmSplitKParallel<ElementInputA,
                                                       LayoutInputA,
                                                       ElementInputB,
                                                       LayoutInputB,
                                                       ElementOutput,
                                                       LayoutOutput,
                                                       ElementAccumulator,
                                                       MMAOp,
                                                       SmArch,
                                                       ShapeMMAThreadBlock,
                                                       ShapeMMAWarp,
                                                       ShapeMMAOp,
                                                       EpilogueOp>;


  const int length_m = 5120;
  const int length_n = 4096;
  const int length_k = 4096;
  // CUTLASS 的所有 GEMM kernel 都使用这个 GemmCoord 来获取问题规模
  cutlass::gemm::GemmCoord problem_size(length_m, length_n, length_k);

// data prepare
  cutlass::HostTensor<ElementInputA, LayoutInputA> tensor_a(problem_size.mk());
  cutlass::HostTensor<ElementInputB, LayoutInputB> tensor_b(problem_size.kn());
  cutlass::HostTensor<ElementOutput, LayoutOutput> tensor_c(problem_size.mn());
  cutlass::HostTensor<ElementOutput, LayoutOutput> tensor_d(problem_size.mn());

  cutlass::reference::host::TensorFillRandomUniform(
      tensor_a.host_view(),
      1,
      ElementInputA(4),
      ElementInputA(-4),
      0);
 

  cutlass::reference::host::TensorFillRandomUniform(
      tensor_b.host_view(),
      1,
      ElementInputB(4),
      ElementInputB(-4),
      0);
  cutlass::reference::host::TensorFillRandomUniform(
      tensor_c.host_view(),
      1,					// seed
      ElementOutput(4),		// 随机数上下限
      ElementOutput(-4),
      0);
// fill with zeros
cutlass::reference::host::TensorFill(tensor_d.host_view());

// H2D
  tensor_a.sync_device();
  tensor_b.sync_device();
  tensor_c.sync_device();
  tensor_d.sync_device();

  // Initialize alpha and beta for dot product computation
  ElementComputeEpilogue alpha = ElementComputeEpilogue(1);
  ElementComputeEpilogue beta = ElementComputeEpilogue(0);

  int split_k_slices = 16;  // 划分成16片

// 创建内核参数，后面传递给实例化的kernel
typename Gemm::Arguments arguments{problem_size,  // problem size
                     tensor_a.device_ref(),  // <- reference to matrix A on device
                     tensor_b.device_ref(),  // <- reference to matrix B on device
                     tensor_c.device_ref(),  // <- reference to matrix C on device
                     tensor_d.device_ref(),  // <- reference to matrix D on device
                     {alpha, beta},          // <- tuple of alpha and beta
                     split_k_slices};        // <- k-dimension split factor

// 根据上述参数，可以获得workspace大小
// 是一块临时内存，用于存储一些中间结果
 size_t workspace_size = Gemm::get_workspace_size(arguments);
// allocate workspace memory
 cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);

// 初始化kernel  
  cutlass::Status status = gemm_op.initialize(arguments, workspace.get());
  CUTLASS_CHECK(status);

// launch 初始化之后的kernel
  status = gemm_op();
```

不同硬件的tensor core会有一些差异

#  07、tensorop_gemm

介绍利用tensor core进行矩阵乘法

这里介绍的新东西包括

* 线程块调度策略`cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>;`在路径下`include/cutlass/gemm/threadblock/threadblock_swizzle.h`下

  * GemmIdentityThreadblockSwizzle
  * GemmHorizontalThreadblockSwizzle：按照n维度排布线程块，适合n >> m的情况
  * GemmBatchedIdentityThreadblockSwizzle：支持batched GEMM调度，支持并行处理多个GEMM
  * GemmSplitKHorizontalThreadblockSwizzle：支持多个block同时计算一个tile，然后进行reduce
  * GemmSplitKIdentityThreadblockSwizzle：同上

* pipeline：流水线`constexpr int NumStages = 2;`控制了每个线程块在执行 GEMM 时，内部共享内存和寄存器之间的数据预取与计算的“交错”程度。`NumStages = 2` 意味着每个 threadblock 会为 **A 和 B tile** 预留两个共享内存“缓冲区”，每次执行时：

  * 一组线程在计算第一个 tile（Stage 0）
  * 另一组线程在加载下一个 tile（Stage 1）

  这样在计算当前 tile 的同时预取下一 tile，实现 **加载和计算重叠**。这就是所谓的 **双缓冲 double-buffered pipeline**

  所有**与共享内存分配、双缓冲机制、warp 内部加载计算调度等底层细节，都是由 CUTLASS 自动管理的**。你完全不需要手动干预

```c++
......
using SwizzleThreadBlock = cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>;

// Number of pipelines you want to use
constexpr int NumStages = 2;

using Gemm = cutlass::gemm::device::Gemm<ElementInputA,
                                         LayoutInputA,
                                         ElementInputB,
                                         LayoutInputB,
                                         ElementOutput,
                                         LayoutOutput,
                                         ElementAccumulator,
                                         MMAOp,
                                         SmArch,
                                         ShapeMMAThreadBlock,
                                         ShapeMMAWarp,
                                         ShapeMMAOp,
                                         EpilogueOp,
                                         SwizzleThreadBlock,
                                         NumStages>;
......

```

# 08、针对turing架构，没什么新的



# 09、介绍了利用Tensor core运行卷积核(turing)

新的东西包括

* 数据布局`using LayoutInputA = cutlass::layout::TensorNHWC;` 是 CUTLASS 中表示 **4D 张量布局**的类，在文件`include/cutlass/layout/tensor.h`，而前面行主序和列主序在这里定义的`include/cutlass/layout/matrix.h`
* 前向卷积(Fprop)内核

```c++
using ThreadblockShape = cutlass::gemm::GemmShape<128, 128, 128>;  // Threadblock tile shape
using WarpShape = cutlass::gemm::GemmShape<64, 64, 128>;         // Warp tile shape
using InstructionShape = cutlass::gemm::GemmShape<8, 8, 32>;    // TensorCore instruction shape

using Conv2dFpropKernel = typename cutlass::conv::kernel::DefaultConv2dFprop<
  ElementInputA, LayoutInputA,
  ElementInputB, LayoutInputB,
  ElementOutput, LayoutOutput,
  ElementAccumulator,
  MMAOp,
  SmArch,
  ThreadblockShape,
  WarpShape,
  InstructionShape,
  EpilogueOp,
  SwizzleThreadBlock,
  NumStages,
  cutlass::arch::OpMultiplyAddSaturate,			// Tensor core 的 fused multiply-add 类型
  cutlass::conv::IteratorAlgorithm::kAnalytic	// 卷积迭代器类型
>::Kernel;
/*
OpMultiplyAddSaturate：指定 Tensor Core 执行 FMA（fused multiply-add） 时使用的 操作类型，(a × b) + c 的结果如果超出目标类型（如 int4, int8）可表示的范围，会自动饱和裁剪（saturate）

kAnalytic：配置 卷积迭代器算法类型 的枚举值，决定 CUTLASS 如何访问输入张量（input tensor A/B），使用 解析式（Analytic） 算法计算每一个卷积窗口的对应位置
*/ 

/*
前面已经通过 DefaultConv2dFprop 完整生成了一个 可用于前向卷积的 low-level kernel，
ImplicitGemmConvolution 会使用这个 kernel 构造一个更高级的 "device operator"，从而让你直接像下面这样使用
ImplicitGemm conv_op;
auto status = conv_op(arguments);
*/
using ImplicitGemm = cutlass::conv::device::ImplicitGemmConvolution<Conv2dFpropKernel>;

// 指定卷积操作的模式
/*
enum class Mode {
  kCrossCorrelation,	// 实际常用的深度学习卷积，实际上数学上叫做“交叉相关（Cross-Correlation）
  kConvolution			// 真正的数学“卷积”：对 Filter 做空间翻转
};
*/
cutlass::conv::Mode mode = cutlass::conv::Mode::kCrossCorrelation;

int split_k_slices = 1;

cutlass::conv::Conv2dProblemSize problem_size(      
    options.input_size,
    options.filter_size,
    options.padding,
    options.conv_stride,
    options.dilation,
    options.output_size(),
    mode,
    split_k_slices);

 typename ImplicitGemm::Arguments arguments{
    problem_size,
    tensor_a.device_ref(),
    tensor_b.device_ref(),
    tensor_c.device_ref(),
    tensor_c.device_ref(),
    {options.alpha, options.beta},
  };

ImplicitGemm implicit_gemm_op;

// workspace
size_t workspace_size = implicit_gemm_op.get_workspace_size(arguments);
cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);
result.status = implicit_gemm_op.can_implement(arguments);
result.status = implicit_gemm_op.initialize(arguments, workspace.get());

// launch
result.status = implicit_gemm_op();
```



# 10/11 复数gemm，hold

# 12、gemm+bias+relu

`D = ReLU(alpha * A × B + Bias)`，其实就是在普通的gemm上面的扩展

```c++
// 数据类型
using ElementAccumulator = float;
using ElementComputeEpilogue = ElementAccumulator;  
using ElementInputA = cutlass::half_t;              
using ElementInputB = cutlass::half_t;              
using ElementOutput = float;          

// layout
using LayoutInputA = cutlass::layout::ColumnMajor;
using LayoutInputB = cutlass::layout::ColumnMajor;
using LayoutOutput = cutlass::layout::ColumnMajor;

// use tensor core
using MMAOp = cutlass::arch::OpClassTensorOp;
using SmArch = cutlass::arch::Sm75;

// tile size
using ShapeMMAThreadBlock = cutlass::gemm::GemmShape<128, 128, 32>;
using ShapeMMAWarp = cutlass::gemm::GemmShape<64, 64, 32>;
using ShapeMMAOp = cutlass::gemm::GemmShape<16, 8, 8>;

// scheduled
using SwizzleThreadBlock = cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>;

// epilogue operation as LinearCombinationRelu
// d_ij = max(0, alpha * sum_k(a_ik * b_kj) + c_ij )

// 注意这里跟gemm的区别是使用的是LinearCombinationRelu
using EpilogueOp = cutlass::epilogue::thread::LinearCombinationRelu<
    ElementOutput,                                       
    128 / cutlass::sizeof_bits<ElementOutput>::value,   
    ElementAccumulator,                                   
    ElementComputeEpilogue,                               
    cutlass::epilogue::thread::ScaleType::NoBetaScaling>; 

constexpr int NumStages = 2;

using Gemm = cutlass::gemm::device::Gemm<ElementInputA,
                                         LayoutInputA,
                                         ElementInputB,
                                         LayoutInputB,
                                         ElementOutput,
                                         LayoutOutput,
                                         ElementAccumulator,
                                         MMAOp,
                                         SmArch,
                                         ShapeMMAThreadBlock,
                                         ShapeMMAWarp,
                                         ShapeMMAOp,
                                         EpilogueOp,
                                         SwizzleThreadBlock,
                                         NumStages>;

// initialize alpha
ElementComputeEpilogue alpha = ElementComputeEpilogue(1);

int split_k_slices = 1;

// tensor_c_bias.device_data()：指向设备端的 bias 数据，它是 Mx1 大小（每一行一个偏置）
// 0：表示列方向（N方向）stride 为 0，也就是说这个 bias 向量是 广播（broadcast）到每一列上的
typename Gemm::Arguments arguments{
    problem_size,                       // <- problem size of matrix multiplication
    tensor_a.device_ref(),              // <- reference to matrix A on device
    tensor_b.device_ref(),              // <- reference to matrix B on device
    {tensor_c_bias.device_data(), 0},   
    tensor_d.device_ref(),              // <- reference to matrix D on device
    {alpha},                              // <- alpha
    split_k_slices};                    // <- k-dimension split factor

size_t workspace_size = Gemm::get_workspace_size(arguments);
cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);
Gemm gemm_op;
cutlass::Status status = gemm_op.can_implement(arguments);
status = gemm_op.initialize(arguments, workspace.get());

status = gemm_op();
```

# 13、hold

# 14、ampere_gemm

跟上面的其他架构的使用方法没有区别，

发现调流水线级数以及til_size真的对性能有很大影响

```
using SmArch = cutlass::arch::Sm80;
```





**因为工作需要，先学习hopper架构相关的例子，前面的慢慢补习**



# 48、warp_specialized_gemm

* Hopper架构引入了一系列新的张量核心指令(GMMA)，效率比Ampere张量核心指令更高
* Hopper包含新的TMA单元，支持G和S之间更高效的数据传输，还支持集群中block之间的异步复制。此外TMA 可以加载 FP32 数据并将其隐式转换为 TF32(*什么是FP32和TF32？*)
* 通过调整CTA栅格化方向和swizzle(洗牌)模式，可以控制线程块之间的访问局部性，从而提高性能。
  * CTA Rasterization Direction（CTA 栅格化方向）：线程块在 2D/3D 网格中是**按行**排列、还是**按列**排列
  * Swizzle Pattern（洗牌模式）：是一种地址重新映射技术，用于将线程块的坐标（例如 2D 的 (x, y)）**重新排列**为另一个顺序，目的是增加缓存命中率或者避免冲突
* CTA栅格化控制

在文件include/cutlass/gemm/kernel/tile_scheduler_params.h中描述，包括诸多选项

调度顺序选项：

```c++
enum class RasterOrderOptions {
    Heuristic,		// 使用 CUTLASS 自带的启发式规则自动决定
    AlongM,			// 优先遍历M维度
    AlongN			// 优先便利N维度
};
```



回头详细阅读https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/efficient_gemm.md

通过这节，主要学习一下在Hopper架构中，怎么写出一个高性能的gemm，并配置可调的CTA栅格方向以及重排模式

```c++
// 与之前不同的是，需要16字节(128位)对齐
using         ElementA    = float; 
using         LayoutA     = cutlass::layout::RowMajor; 
constexpr int AlignmentA  = 128 / cutlass::sizeof_bits<ElementA>::value;  

using         ElementB    = float;
using         LayoutB     = cutlass::layout::ColumnMajor; 
constexpr int AlignmentB  = 128 / cutlass::sizeof_bits<ElementB>::value;

using         ElementC    = float;
using         LayoutC     = cutlass::layout::ColumnMajor;
constexpr int AlignmentC  = 128 / cutlass::sizeof_bits<ElementC>::value;

// 内核配置
using ElementAccumulator  = float;  // 内部计算用float
using ArchTag             = cutlass::arch::Sm90; // 最低sm90
using OperatorClass       = cutlass::arch::OpClassTensorOp;  // 使用tensorcore
using TileShape           = Shape<_128,_128,_32>;   // block级别的tile_size
using ClusterShape        = Shape<_4,_2,_1>;  // 每个 CTA 集群（threadblock cluster）是 4x2x1 的分布结构，适配 Hopper 的 cluster scheduling 特性。
using StageCountType = cutlass::gemm::collective::StageCountAuto;   
using KernelSchedule = cutlass::gemm::collective::KernelScheduleAuto;

// 使用cutlass的collectiveBuilder构建epilogue和mainloop部分，结合 TileShape 和 ClusterShape。
// 会自动决策寄存器和共享内存的占用等
using CollectiveEpilogue = typename cutlass::epilogue::collective::CollectiveBuilder<
    cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
    TileShape, ClusterShape,
    cutlass::epilogue::collective::EpilogueTileAuto,
    ElementAccumulator, ElementAccumulator,
    ElementC, LayoutC, AlignmentC,
    ElementC, LayoutC, AlignmentC,
    cutlass::epilogue::collective::EpilogueScheduleAuto
  >::CollectiveOp;

using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
    ArchTag, OperatorClass,
    ElementA, LayoutA, AlignmentA,
    ElementB, LayoutB, AlignmentB,
    ElementAccumulator,
    TileShape, ClusterShape,
    cutlass::gemm::collective::StageCountAutoCarveout<
      static_cast<int>(sizeof(typename CollectiveEpilogue::SharedStorage))>,
    cutlass::gemm::collective::KernelScheduleAuto
  >::CollectiveOp;

// 一个gemm问题就被简单描述成了3部分：problem size、mainloop和epilogue
// GemmUniversalAdapter 是一种适配器，支持多种 GEMM 模式（包括 Batched、Strided、Universal 等）
using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
    Shape<int,int,int>, // Indicates ProblemShape
    CollectiveMainloop,
    CollectiveEpilogue
>;
using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;


using StrideA = typename Gemm::GemmKernel::StrideA;
using StrideB = typename Gemm::GemmKernel::StrideB;
using StrideC = typename Gemm::GemmKernel::StrideC;
using StrideD = typename Gemm::GemmKernel::StrideD;

StrideA stride_A;
StrideB stride_B;
StrideC stride_C;
StrideD stride_D;
uint64_t seed;

cutlass::DeviceAllocation<typename Gemm::ElementA> block_A;
cutlass::DeviceAllocation<typename Gemm::ElementB> block_B;
cutlass::DeviceAllocation<typename Gemm::ElementC> block_C;
cutlass::DeviceAllocation<typename Gemm::EpilogueOutputOp::ElementOutput> block_D;

using RasterOrderOptions = typename cutlass::gemm::kernel::detail::PersistentTileSchedulerSm90Params::RasterOrderOptions;


RasterOrderOptions raster;
// 使用 CUTLASS 自带的启发式规则自动决定
raster(RasterOrderOptions::Heuristic)
    
// 初始化数据,设置张量的步幅，
stride_A = cutlass::make_cute_packed_stride(StrideA{}, {options.m, options.k, 1});
stride_B = cutlass::make_cute_packed_stride(StrideB{}, {options.n, options.k, 1});
stride_C = cutlass::make_cute_packed_stride(StrideC{}, {options.m, options.n, 1});
stride_D = cutlass::make_cute_packed_stride(StrideD{}, {options.m, options.n, 1});

// 设置device_id
int device_id = 0;
cutlass::KernelHardwareInfo kernel_hw_info = 	cutlass::KernelHardwareInfo::make_kernel_hardware_info<Gemm::GemmKernel>(device_id);

typename Gemm::Arguments arguments{
cutlass::gemm::GemmUniversalMode::kGemm,
{options.m, options.n, options.k},
{block_A.get(), stride_A, block_B.get(), stride_B}, // 输入矩阵 A、B 数据指针和步幅
{{options.alpha, options.beta}, block_C.get(), stride_C, block_D.get(), stride_D},
kernel_hw_info		// 可选的硬件调度信息
};

// 控制 CUTLASS 如何在网格中 遍历 tile（矩形块）
arguments.scheduler.raster_order = options.raster;
// 用于 scheduler 的 tile swizzling（平铺重排）策略：
// 避免线程块访问冲突，提高调度均衡性
arguments.scheduler.max_swizzle_size = options.swizzle;

Gemm gemm;
size_t workspace_size = Gemm::get_workspace_size(arguments);
cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);

// Check if the problem size is supported or not
CUTLASS_CHECK(gemm.can_implement(arguments));

// Initialize CUTLASS kernel with arguments and workspace pointer
CUTLASS_CHECK(gemm.initialize(arguments, workspace.get()));

CUTLASS_CHECK(gemm.run());

```

**总结一下hopper架构gemm的使用**

1. 数据定义：类型、布局和对齐
2. 计算配置：内部计算类型、arch、tensor core、tile size等
3. Mainloop & Epilogue 构建
4. 实例化gemm kernel （problem size、mainloop、epilogue）
5. 构建内核所需参数(输入输出)以及raster和swizzle配置

**Hopper架构默认使用了warp specialized设计，这部分代码后面再详细阅读**



# 49 collective_builder

展示了如何使用 CUTLASS 的 CollectiveBuilder 轻松构建针对 NVIDIA Hopper 架构的高性能内核

背景和动机：

在cutlass 2中，gemm内核是通过大量的模板参数来配置的，包括一些数据配置以及计算配置，为了简化使用，引入了`DefaultGemmConfigurations`，只需要给定少量的参数比如数据类型和GPU架构，就可以自动设置其他模板参数(比如warp数量，流水线阶段数等)。

存在的问题包括：

* 不灵活，修改一个参数需要给出其他所有需要配置的参数，不能部分修改
* 重复代码多，每种类型组合都要写一套`DefaultGemmConfiguration`,难以扩展
* hopper支持更复杂的配置
  * 数据加载策略(hopper的TMA vs Ampere的cp.async)
  * warp 内的工作划分方式（如 warp specialization）
  * 是否使用 persistent thread blocks

为了应对上面出现的问题，CUTLASS 3 引入了新的配置方式：CollectiveBuilder

特点：

* 和`DefaultGemmConfigurations`一样，用户只需要提供少量的参数进行配置
* 它**自动选择最佳的数据加载方式**（是否用 TMA），调度策略（调度 policy）、stage count 等
* 它使用 `Auto` 模板参数作为占位符，代表你希望系统来自动决定最优参数
* 它不需要为每种类型组合写专门的配置，而是根据数据类型、布局、线程块大小等通用属性**“构建”**出合理配置

例如，当stage count设置为anto的时候，会根据 shared memory 大小、数据类型和线程块形状，自动推导一个合适的值，而不是固定写死为 3（像 CUTLASS 2 那样）

注意：

* cutlass支持为mainloop和epilogue提供builder，使用auto的好处是可以享受版本升级，但是无法保证一致性。mainloop和epilogue必须同时用或者不用auto

* CollectiveBuilder并非强制的，依然可以通过显式指定模板参数来定义内核

**看代码**

```c++
  using LayoutA = cutlass::layout::RowMajor;
  using LayoutB = cutlass::layout::ColumnMajor;
  using LayoutC = cutlass::layout::ColumnMajor;
  using LayoutD = cutlass::layout::ColumnMajor;

  using ElementA = cutlass::half_t;
  using ElementB = cutlass::half_t;
  using ElementC = cutlass::half_t;
  using ElementD = cutlass::half_t;
  using ElementAccumulator = float;
  using ElementCompute = float;
  using ElementScalar = float;

  // 16B alignment lets us use TMA
  static constexpr int AlignmentA = 16 / sizeof(ElementA);
  static constexpr int AlignmentB = 16 / sizeof(ElementB);
  static constexpr int AlignmentC = 16 / sizeof(ElementC);
  static constexpr int AlignmentD = 16 / sizeof(ElementD);

  static_assert(not UseCustomEVT ||
    (cute::is_same_v<EpilogueScheduleType, cutlass::epilogue::TmaWarpSpecialized> ||
     cute::is_same_v<EpilogueScheduleType, cutlass::epilogue::TmaWarpSpecializedCooperative>),
    "Epilogue visitor trees are currently only supported by the TMA warp-specialized epilogue");
  static constexpr auto RoundStyle = cutlass::FloatRoundStyle::round_to_nearest;

/*
这是 CUTLASS 3 的 Epilogue Fusion 功能：允许用户定义复杂的输出处理操作（例如 alpha * acc + beta * C）。这段使用了嵌套结构的 visitor tree 构建出类似于：D = alpha * acc + beta * C
也可以使用默认操作DefaultOperation
通过 cute::conditional_t<UseCustomEVT, CustomEVT, DefaultOperation> 动态决定使用哪一种

*/
  using CustomEVT =  // alpha * acc + beta * C
    cutlass::epilogue::fusion::Sm90EVT<cutlass::epilogue::fusion::Sm90Compute<cutlass::homogeneous_multiply_add, ElementD, ElementCompute, RoundStyle>, // beta * C + (alpha * acc)
      cutlass::epilogue::fusion::Sm90ScalarBroadcast<ElementScalar>, // beta
      cutlass::epilogue::fusion::Sm90SrcFetch<ElementC>, // C
      cutlass::epilogue::fusion::Sm90EVT<cutlass::epilogue::fusion::Sm90Compute<cutlass::multiplies, ElementCompute, ElementCompute, RoundStyle>, // alpha * acc
        cutlass::epilogue::fusion::Sm90ScalarBroadcast<ElementScalar>, // alpha
        cutlass::epilogue::fusion::Sm90AccFetch // acc
      >
    >;

using DefaultOperation = cutlass::epilogue::fusion::LinearCombination<ElementD, ElementCompute, ElementC, ElementScalar, RoundStyle>;

// CollectiveBuilder 构建内核组件
  using CollectiveEpilogue = typename cutlass::epilogue::collective::CollectiveBuilder<
      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
      Shape<_128,_128,_64>, Shape<_1,_1,_1>,
      cutlass::epilogue::collective::EpilogueTileAuto,
      ElementAccumulator, ElementCompute,
      ElementC, LayoutC, AlignmentC,
      ElementD, LayoutD, AlignmentD,
      EpilogueScheduleType,
      cute::conditional_t<UseCustomEVT, CustomEVT, DefaultOperation>
    >::CollectiveOp;

// 它接受多个参数来推导出具体的内核实现
/*
cutlass::arch::Sm90,              // 目标架构：Hopper (SM90)
cutlass::arch::OpClassTensorOp,  // 使用 Tensor Core（不是 SIMT）
ElementA, LayoutA, AlignmentA,   // A矩阵的数据类型、内存布局、对齐字节数
ElementB, LayoutB, AlignmentB,   // B矩阵的数据类型、内存布局、对齐字节数
ElementAccumulator,				// 中间累加器类型
Shape<_128,_128,_64>,     // threadblock tile size：M = 128, N = 128, K = 64
Shape<_2,_1,_1>,          // cluster shape：2 个 threadblock 在 M 方向聚合
cute::conditional_t<...>	// 自动计算流水线或者指定流水线深度
MainloopScheduleType		// 主循环调度策略
*/
  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
      ElementA, LayoutA, AlignmentA,
      ElementB, LayoutB, AlignmentB,
      ElementAccumulator,
      Shape<_128,_128,_64>, Shape<_2,_1,_1>,
      cute::conditional_t<cute::is_same_v<StageCountType, cutlass::gemm::collective::StageCountAuto>,
          cutlass::gemm::collective::StageCountAutoCarveout<static_cast<int>(sizeof(typename CollectiveEpilogue::SharedStorage))>,
          StageCountType>,
      MainloopScheduleType
    >::CollectiveOp;

// 内核组装
  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
      Shape<int,int,int,int>,
      CollectiveMainloop,
      CollectiveEpilogue,
      TileSchedulerType			// 模板参数传进去的cutlass::gemm::PersistentScheduler
  >;
  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;

// 初始化函数initialize()
/*
为矩阵 A/B/C/D 分配设备内存（通过 block_X.reset()）
设置各个矩阵的 stride（步长）
初始化 block 数据（通过 initialize_block）
*/
  void initialize(const ProblemShapeType& problem_size) {
    auto problem_shape_MNKL = cute::append<4>(problem_size, 1);
    auto [M, N, K, L] = problem_shape_MNKL;

    stride_A = cutlass::make_cute_packed_stride(StrideA{}, cute::make_shape(M, K, L));
    stride_B = cutlass::make_cute_packed_stride(StrideB{}, cute::make_shape(N, K, L));
    stride_C = cutlass::make_cute_packed_stride(StrideC{}, cute::make_shape(M, N, L));
    stride_D = cutlass::make_cute_packed_stride(StrideD{}, cute::make_shape(M, N, L));

    block_A.reset(M * K * L);
    block_B.reset(K * N * L);
    block_C.reset(M * N * L);
    block_D.reset(M * N * L);
    block_ref_D.reset(M * N * L);

    initialize_block(block_A, seed + 2023);
    initialize_block(block_B, seed + 2022);
    initialize_block(block_C, seed + 2021);
  }

// 构建kernel参数
    typename Gemm::Arguments arguments{
      cutlass::gemm::GemmUniversalMode::kGemm,
      problem_size,
      {block_A.get(), stride_A, block_B.get(), stride_B},
      {{}, // epilogue.thread
       block_C.get(), stride_C, block_D.get(), stride_D},
      hw_info
    };

  Gemm gemm_op;
  size_t workspace_size = Gemm::get_workspace_size(arguments);
  cutlass::Status status = gemm_op.can_implement(arguments);
  status = gemm_op.initialize(arguments, workspace.get());
  
// run the gemm
  status = gemm_op.run();
  cudaError_t result = cudaDeviceSynchronize();
```

* 待熟悉的点：(**TODO**)
  * 各种调度方式的实现 include/cutlass/gemm/dispatch_policy.hpp
  * 流水线pipeline include/cutlass/pipeline



明天学 pipeline，完全熟悉
