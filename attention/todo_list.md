- attention

+ [ ] self attention
+ [ ] online attention
+ [ ] flash attention
+ [ ] falsh attention 2
+ [ ] flash attention 3
+ [ ] falsh decoding
+ [ ] flash decoding ++
+ [ ] scaled dot-product attention（SDPA）
+ [ ] multi-head self-attention（MHSA）

  + [ ] multi-head attention (MHA)
  + [ ] grouped-query attention (GQA)
  + [ ] multi-query attention (MQA)
  + [ ] multi-head latent attention (MLA)
  + [ ] multi-token attention (MTA)
  + [ ] sage attention 1
  + [ ] sage attention 2
  + [ ] sage attention 2++
  + [ ] sage attention 3
  + [ ] paged attention
  + [ ] ring attention
  + [ ] ring flash attention
  + [ ] linear attention
  + [ ] lightning attention
  + [ ] native sparse attention (NSA)
  + [ ] grouped latent attention (GLA)
  + [ ] grouped-tied attention (GTA)