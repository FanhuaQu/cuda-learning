# 一、前言

单卡的内存是有限的，可以分为两个维度来看

* 数据维度：小模型虽然可以放在一张卡上，但是可能显存放不下一个batch所需要的全部数据所以数据需要切分到多张卡上计算并最终汇总，这就引入了**数据并行DP**
* 模型维度，对于大模型而言，单卡放不下一整个模型，这就需要将模型切分到多张卡上，由此引入了(TP/PP/EP/SP)，统称为模型并行

对于小模型而言，如果单个batch数据单卡放不下，只需要DP，使用各种模型并行反而会降低计算效率

不同并行之间的依赖关系：**EP依赖于DP，SP依赖于TP**  



# 二、各种并行方案

## 2.1 数据并行(Data Parallelism)

数据并行最简单，每张卡拷贝相同的模型结构，仅对数据做切分。每张卡计算完的梯度也是针对各自数据的，需要做一次allreduce，然后使用优化器更新模型，进入下一次迭代。数学基础如下

![image-20250818005439187](C:\Users\Qufanhua\AppData\Roaming\Typora\typora-user-images\image-20250818005439187.png)

**优点：**是所有并行中效率最高的，只需要进行一次allreduce操作，通信量是`2*M*dtype`(reduce-scatter + all-gather)

**缺点：**单纯的DP只适用于小模型，举个例子对于llama-7B的bf16混合训练，显存占用最少是`7B * (2 + 2 + (4 + 4 + 4)) = 112G`,其中模型和梯度各占2byte，adam优化器占12byte。显然主流的H100(80G)放不下，需要引入模型并行。(怎么算的：Adam优化器状态包括FP32 master weight、m(一阶动量)、v(二阶动量))

由于DP效率最高，DP为所有并行中首选且必带的。同时只要一张卡放不下一个batch中的数据（以及其产生的激活值），**DP也是必选的**

## 2.1  张量并行(Tensor Parallelism)

张量并行是指对模型内部的参数矩阵切分，然后利用分块矩阵乘进行计算得到正确结果，由于参数矩阵是以tensor表示，故叫张量并行.

**优点：**是能分摊模型到多张卡上

**缺点：**是带来了不小的通信开销，影响训练效率

以transfomer为例，理解TP是怎么回事

1. **embeding部分**：假设embeding的大小为`vocab * hidden`，GPU包含`N`卡，则每张卡分到`vocab/N * hidden`；embeding部分做的工作是查找，每张卡拿到所有的input_ids并查找input_id是否在embeding中，没有设置为0。查找完毕后做一次allreduce，通信开销为`2 * b * s * h`，b是batch、s是seq_len、h是hidden_size。（怎么理解：首先输入形状是[b,s]。embedding矩阵大小是[vocab, h]，会按行切分成p份，每张卡存的都是部分的embedding。这里查embedding的话只在本卡拥有的词片区间查表，不在本卡范围内的id返回全零向量。输出形状是[b, s, h]，里面会有一些全零行。对[b, s, h]做一次all-reduce，把分散在各卡的稀疏非零值相加，得到完整的正确的embedding。这个很好理解，因为词表维度很大，行切能把参数均匀分散到各卡，对于[b,s,h]通信量是`2*(p-1)/p*b*s*h`）

   ![image-20250902014921554](C:\Users\Qufanhua\AppData\Roaming\Typora\typora-user-images\image-20250902014921554.png)

2. **attention部分**

   1. mha：直接按heads进行tp_size切分，heads必须能被tp_size整除
   2. gqa：对于query group部分进行tp_size切分，query_groups必须能被tp_size整除
   3. output_layer：将output_layer按行划分，mha/gqa的结果经过tp切分后本质是按列划分，和对应的output_layer按行划分后，相乘的结果最后allreduce
   4. 额外带来的通信开销是：2 * b * s * h

   (**怎么理解**？首先搞清楚维度，QKV 线性层W_qkv大小为[h,3h]，按列切分成p份，每张卡[h,3h/p]，等效于把**若干 heads（或 query groups）分给每卡**。输出投影，W_o按行切成p份，每张卡[h/p, h]，前一步的输出张量是[b,s,h/p]，与输出投影乘积为[b,s,h]，这个时候需要做一次allreduce，把部分和结果合并)

   ![image-20250902014934787](C:\Users\Qufanhua\AppData\Roaming\Typora\typora-user-images\image-20250902014934787.png)

3. **MLP部分(FFN)**

   1. 输入不变，第一个矩阵（h*4h）按列切，每张卡运算的结果为`b*s*4h/tp`，其值进行激活运算，第二个矩阵(`4h*h`)按照行切，乘法之后的结果是`b*s*h`，对这个结果做allreduce，通信开销是`2*b*s*h`
   1. 为什么不将第一个矩阵按行切第二个矩阵按列切？因为这样第一个矩阵的结果就是`b*s*4h`，在激活值之前还需要做allreduce，没有节省空间，还有额外的通信

   （**怎么理解？**假设第一层权重w1是[h,4h]，第二层权重w2是[4h,h]，标准的切法是w1按列切分，每张卡权重是[h,4h/p]，前向传播得到局部的中间激活值[b, s, 4h/p]。第二层权重w2按照行切，前向传播输出为[b,s, h]，需要做all-reduce操作得到完整的输出。解释一下为什么不反过来，如果w1行切,每张卡[h/p,4h]，输入应该也要行切，输出就是）

   ![image-20250902015014892](C:\Users\Qufanhua\AppData\Roaming\Typora\typora-user-images\image-20250902015014892.png)

4. **output_layer部分：**

   1. output_layer是`h*vocab_size`，输入是`b*s*h`
   2. 将output_layer按列划分，每张卡的结果为`(b*s)*(vocab_size/tp)`，由于最后结果算loss时需要交叉熵，要对每一行做softmax，因此还需要再做一次allreduce。由于vocab_size很大(>100k)，这个通信量就很夸张了，一种优化方式是先将每张卡上的结果按行sum，得到的结果是`b*s`，对求和的结果做allreduce，通信量是`b*s*tp`

   （**怎么理解？**，output_layer层的输入是[b*s, h]，这里做了展开，W_vocab 大小是[`h, vocab_size`]，按列切分，每张卡数据是[`h,vocab_size/tp`]，结算局部结果为[`b*s, vocab_size/tp`]。这里提到的计算交叉熵问题是：softmax需要对全词表做exp求和，如果把1前面[`b*s, vocab_size/tp`]结果聚合起来成本非常大。因此实际实现采用 **数值稳定的对数-求和-指数（log-sum-exp）两次小规约**：**全局行最大值**：先在本地求每行最大 `m_local : [b*s]`，再 **All-Reduce(max)** 得到 `m_global : [b*s]`。**全局行和**：用 `m_global` 做偏移，计算本地 `sum_local = Σ exp(logits_local - m_global)`（仍是 `[b*s]`），再 **All-Reduce(sum)** 得到 `sum_global : [b*s]`。**目标类别 logit**：如果标签在本卡的词片上，直接取；不在，则常用一次 **All-Gather/All-Reduce on picked values** 只在 `[b*s]` 规模上交换**目标 logit**（而不是整行）。用 `m_global` 与 `sum_global` 组合出 `log_softmax`，计算 NLLLoss）

5. 张量并行的通信开销是$o(b*s*h*l)$，其中$l$是层数。张量并行一般仅在单机8卡内做，通信量太大了

![image-20250902015249441](C:\Users\Qufanhua\AppData\Roaming\Typora\typora-user-images\image-20250902015249441.png)

## 2.3 流水线并行(Pipeline Parallelism)

* 前面提到的张量并行是对矩阵进行切分，缺点是通信量很大，一般只用于单机8卡
* 另一种思路是按照Layer进行划分，加入一个模型有80层，一张卡放不下，但假如切分到8张卡上面，每张卡上面放10层，这样就避免了TP那样频繁的通信量。通信发生在层与层之间，例如前面forward阶段前10层计算完之后将数据传递给下一张卡的后面10层，这样就有一个问题，会出现等待。流水线并行要解决的最大的难题就是尽量减少等待的时间，也就是对应timeline里面的空泡，如果计算气泡率的话是$\frac{p-1}{p}$，其中p是流水线阶段数，就是rank的数量。这个很好理解，任意时刻都只有一张卡在工作。

![image-20250903011057125](C:\Users\Qufanhua\AppData\Roaming\Typora\typora-user-images\image-20250903011057125.png)

* 减少气泡的一个方法是使用micro-batch，假如只用一个batch，流水线会空转。把大的batch拆分成多个micro-batch。我们再来计算一下这种情况的空泡率。假如总共流水线级数是p，micro batch数量为m，每个micro batch在每张卡上的前向传播耗时$t_{f}$，反向传播耗时为$t_{b}$，每个micro batch在一个step里面都需要做一次前向传播和一次反向传播，有用的时间是$m*p*(t_f+t_b)$。总的时间是$p(m+p-1)(t_f+t_b)$。这样就可以计算得到气泡率如下，会发现m越大气泡率越低，当m>>p的时候，气泡率接近0，所有的卡接近满载
  $$
  气泡率 =1-\frac{m}{m+p-1}\\
  =\frac{p-1}{m+p-1}
  $$
  

![image-20250903013349876](C:\Users\Qufanhua\AppData\Roaming\Typora\typora-user-images\image-20250903013349876.png)

* 那问题来了，理论分析micro batch越多气泡率越低，那是不是可以无脑增加呢？**答案是不是的**。

  * 单个micro batch太小，矩阵乘法效率会很低，并且每步更新的梯度更加noisy
  * 通信开销会变得很大
  * 对于1F1B这样的，太多的micro batch会增加很多的调度开销，因为要维护更多的中间状态。
  * 可能保证m = 2p-4p是比较合适的范围

  











Q：什么是优化器，什么是权重和激活值



ref: 

https://zhuanlan.zhihu.com/p/1904506837543420662