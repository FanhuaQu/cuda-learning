# 一、前言

单卡的内存是有限的，可以分为两个维度来看

* 数据维度：小模型虽然可以放在一张卡上，但是可能显存放不下一个batch所需要的全部数据所以数据需要切分到多张卡上计算并最终汇总，这就引入了**数据并行DP**
* 模型维度，对于大模型而言，单卡放不下一整个模型，这就需要将模型切分到多张卡上，由此引入了(TP/PP/EP/SP)，统称为模型并行

对于小模型而言，如果单个batch数据单卡放不下，只需要DP，使用各种模型并行反而会降低计算效率

不同并行之间的依赖关系：**EP依赖于DP，SP依赖于TP**  



# 二、各种并行方案

## 2.1 数据并行(Data Parallelism)

数据并行最简单，每张卡拷贝相同的模型结构，仅对数据做切分。每张卡计算完的梯度也是针对各自数据的，需要做一次allreduce，然后使用优化器更新模型，进入下一次迭代。数学基础如下

![image-20250818005439187](C:\Users\Qufanhua\AppData\Roaming\Typora\typora-user-images\image-20250818005439187.png)

**优点：**是所有并行中效率最高的，只需要进行一次allreduce操作，通信量是`2*M*dtype`(reduce-scatter + all-gather)

**缺点：**单纯的DP只适用于小模型，举个例子对于llama-7B的bf16混合训练，显存占用最少是`7B * (2 + 2 + (4 + 4 + 4)) = 112G`,其中模型和梯度各占2byte，adam优化器占12byte。显然主流的H100(80G)放不下，需要引入模型并行。(怎么算的：Adam优化器状态包括FP32 master weight、m(一阶动量)、v(二阶动量))

由于DP效率最高，DP为所有并行中首选且必带的。同时只要一张卡放不下一个batch中的数据（以及其产生的激活值），**DP也是必选的**

## 2.1  张量并行(Tensor Parallelism)

张量并行是指对模型内部的参数矩阵切分，然后利用分块矩阵乘进行计算得到正确结果，由于参数矩阵是以tensor表示，故叫张量并行.

**优点：**是能分摊模型到多张卡上

**缺点：**是带来了不小的通信开销，影响训练效率

以transfomer为例，理解TP是怎么回事

1. **embeding部分**：假设embeding的大小为`vocab * hidden`，GPU包含`N`卡，则每张卡分到`vocab/N * hidden`；embeding部分做的工作是查找，每张卡拿到所有的input_ids并查找input_id是否在embeding中，没有设置为0。查找完毕后做一次allreduce，通信开销为`2 * b * s * h`，b是batch、s是seqlen、h是hidden_size
2. **attention部分**
   1. mha：直接按heads进行tp_size切分，heads必须能被tp_size整除
   2. gqa：对于query group部分进行tp_size切分，query_groups必须能被tp_size整除
   3. output_layer：将output_layer按行划分，mha/gqa的结果经过tp切分后本质是按列划分，和对应的output_layer按行划分后，相乘的结果最后allreduce
   4. 额外带来的通信开销是：2 * b * s * h
3. **MLP部分**
   1. 



Q：什么是优化器，什么是权重和激活值























ref: 

https://zhuanlan.zhihu.com/p/1904506837543420662