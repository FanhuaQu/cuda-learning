在同目录下的存储笔记中，记录了模型训练中需要存储的东西以及占用显存量的计算

在训练过程中，很多参数并不是时时刻刻都用到的，例如：

* adam优化器的states($$m_t、v_t$$)只在最后做update的时候用到
* dp的时候，gradients只在最后做update的时候才用到
* 参数W只在做fwd和bwd那一刻才用到
* ......

Zero的思想就是，将这部分参数用完之后就删除，到需要用到的时候再拉过来，这样就节省了宝贵的显存资源

沿着这个思路，递进来看Zero是怎么做存储优化的

# 1. $$P_{os}$$: 优化状态分割

将优化器状态分成若干份，每个GPU上维护一份，减小了这部分的显存开销

<img src="D:\日拱一卒日新月异\cuda编程\gitnote\cuda-learning\大模型训练\assets\image-20251013000627797.png" alt="image-20251013000627797" style="zoom:60%;" />

整体流程如下：

* 每块GPU上存完整的参数W，将一个batch分成3份，每块GPU吃一份，做完一轮fwd和bwd之后，得到各自的梯度G
* 对梯度做一次AllReduce，得到完整的梯度G，通信量是$$2\Phi$$，在Zero1的早期代码中，这里用的是AllReduce，实际上因为每张卡只有部分优化器参数，梯度只做ReduceScatter就行了，这样的通信量是$$\Phi$$。(梯度的作用就是更新优化器参数，所以只需要用于更新优化器参数的部分梯度就行)
* 得到完整的梯度之后，更新本地的那部分优化器参数，然后由优化器参数更新部分权重W(**注意：**这里更新的是fp32的参数，fp16的参数是由fp32 cast得到的)
* 对W做一次All-Gather，在这之前做了`fp16._copy(fp32)`，这样每张卡就拿到了完整的权重，能够进行正常地fwd阶段了。通信量增加是$$\Phi$$

分析一下优化前后的显存占用和单卡通信量(Adam: K = 12)

* 朴素的DP：显存占用$$(2 + 2 + K)\Phi$$，单卡通信量$$2\Phi$$
* $$P_{os}$$：显存占用$$(2 + 2 + \frac{K}{N_d})\Phi$$，单卡通信量$$\Phi$$

假设$$\Phi=7.5B$$，$$N_d=64$$，显存占用从120GB将为了31.4GB

Zero的思想是用通信换显存，类似的思想随处可见！



# 2. $$P_{os} + P_g$$：分割优化器状态和梯度

前面提到，由于每张卡只存了一部分的优化器参数，更新这部分参数也只需要用到对应部分的梯度，因此可以将梯度部分也进行分割，如下图：

<img src="D:\日拱一卒日新月异\cuda编程\gitnote\cuda-learning\大模型训练\assets\image-20251014003943333.png" alt="image-20251014003943333" style="zoom:67%;" />

整体流程如下：

1. 每块GPU上存完整的参数W，一个batch数据分成3份，每个GPU吃一份，fwd和bwd之后，各自的到一份梯度。
2. 对梯度做Reduce-scatter，只需要让每张卡拿到自己需要的那部分梯度的reduce的结果就行，也就是下面的绿色部分，而白色部分在做完ring之后可以丢弃。通信量是$$\Phi$$
3. 每块GPU用自己的O和G去更新相应的W，然后对W做一次All-Gather，单卡通信量是$$\Phi$$

这样显存占用计算公式变成了(120GB —> 31.4GB —> 16.6GB)

**存储降低了8倍，单卡通信量不变！**
$$
(2 + \frac{2 + K}{N_d})\Phi
$$
<img src="D:\日拱一卒日新月异\cuda编程\gitnote\cuda-learning\大模型训练\assets\image-20251014004506331.png" alt="image-20251014004506331" style="zoom:60%;" />

# 3. $$P_{os} + P_g + P_p$$分割优化状态、梯度和参数

**ZeRO的思想就是：万物皆可切，万物皆可抛**，既然如此，何不把参数也切分一下？

<img src="D:\日拱一卒日新月异\cuda编程\gitnote\cuda-learning\大模型训练\assets\image-20251014005218435.png" alt="image-20251014005218435" style="zoom:50%;" />

训练流程如下：

1. 每块GPU上只保存部分参数W。将一个batch的数据分成3份，每块GPU各吃一份
2. 做forward时，对W做一次**All-Gather**，取回分布在别的GPU上的W，得到一份完整的W，单卡通讯量 $$\Phi$$**。forward做完，立刻把不是自己维护的W抛弃**
3. 做backward时，对W做一次**All-Gather**，取回完整的W，单卡通讯量 $$\Phi$$ **。backward做完，立刻把不是自己维护的W抛弃**
4. 做完backward，算得一份完整的梯度G，对G做一次**Reduce-Scatter**，从别的GPU上聚合自己维护的那部分梯度，单卡通讯量 $$\Phi$$**。聚合操作结束后，立刻把不是自己维护的G抛弃**。
5. 用自己维护的O和G，更新W。由于只维护部分W，因此无需再对W做任何操作

这样显存变化为（1.9GB），通信量$$3\Phi$$

到这一步，**我们用1.5倍的通讯开销，换回近120倍的显存**。只要梯度计算和异步更新做的好，通讯时间大部分可以被计算时间隐藏，因此这样的额外通讯开销，也是划算的
$$
(\frac{2+2+K}{N_d})\Phi
$$


Zero和模型并行的区别：

其实**ZeRO是模型并行的形式，数据并行的实质**。
模型并行，是指在forward和backward的过程中，我只需要用自己维护的那块W来计算就行。即**同样的输入X，每块GPU上各算模型的一部分，最后通过某些方式聚合结果**

但对ZeRO来说，它做forward和backward的时候，是需要把各GPU上维护的W聚合起来的，即本质上还是用完整的W进行计算。**它是不同的输入X，完整的参数W，最终再做聚合**





参考：

https://zhuanlan.zhihu.com/p/618865052



